{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b97b2c66",
   "metadata": {},
   "source": [
    "# Finetuning Google's Gemma with Custom Data\n",
    "\n",
    "This notebook follows a [Tutorial on YouTube](https://www.youtube.com/watch?v=iOdFUJiB0Zc).\n",
    "\n",
    "This code walks through setting up Google's Gemma model, along with custom data, for fine-tuning, as well as some queries and results.\n",
    "\n",
    "### Installing Packages\n",
    "\n",
    "Run the following command to install the necessary packages: `pip install requirements.txt`\n",
    "\n",
    "## I. Preparing the Data\n",
    "\n",
    "For this example, I am using a dataset of ArXiv papers (titles, authors, abstracts, etc.). It's a simple dataset that is similar to my interests (AI analysis of research documents)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7056a02a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Benjamin\\.venv\\lm-311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "W1120 21:07:18.872000 3620 Lib\\site-packages\\torch\\distributed\\elastic\\multiprocessing\\redirects.py:29] NOTE: Redirects are currently not supported in Windows or MacOs.\n",
      "Downloading data: 100%|██████████| 1.70G/1.70G [00:44<00:00, 38.4MB/s]\n",
      "Generating train split: 2549619 examples [00:38, 65768.55 examples/s] \n"
     ]
    }
   ],
   "source": [
    "# Import packages\n",
    "import os\n",
    "import transformers\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from trl import SFTTrainer\n",
    "from peft import LoraConfig\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, GemmaTokenizer\n",
    "\n",
    "# Login using e.g. `huggingface-cli login` to access this dataset\n",
    "ds = load_dataset(\"nick007x/arxiv-papers\", token=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34537335",
   "metadata": {},
   "source": [
    "### Dataset Satatistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6c8b5d01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'datasets.dataset_dict.DatasetDict'>\n",
      "{'train': ['arxiv_id', 'title', 'authors', 'submission_date', 'comments', 'primary_subject', 'subjects', 'doi', 'abstract', 'file_path']}\n",
      "{'train': 2549619}\n"
     ]
    }
   ],
   "source": [
    "print(type(ds))\n",
    "print(ds.column_names)\n",
    "print(ds.num_rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "117910d1",
   "metadata": {},
   "source": [
    "# II. Setting Up The Model\n",
    "\n",
    "For this example, I will be using Gemma-2B. This follows the aforementioned YouTube tutorial. Using the smaller model yields lower training and inference time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "32a493b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"google/gemma-2b\"\n",
    "# Disable BNB config if running on CPU - only supported on CUDA GPUs\n",
    "#bnb_config = BitsAndBytesConfig(\n",
    "#    load_in_4bit=True,\n",
    "#    bnb_4bit_quant_type=\"nf4\",\n",
    "#    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "#)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a5ac76b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [02:23<00:00, 71.59s/it] \n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_id, token=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    # quantization_config=bnb_config,   # Disable if running on CPU\n",
    "    # device_map={\"\":0},                # Disable if running on CPU\n",
    "    token=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "166375ea",
   "metadata": {},
   "source": [
    "### Sample Query Before Any Finetuning\n",
    "\n",
    "Giving the model a paper title does result in coherent text. To the untrained eye, it might even look like a proper abstract. It even generates some inline math equations. However, none of the information provided in the dataset is included in the answer - the answer is a complete hallucenation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3dccb20b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The gravitational wave background from star-massive black hole fly-bys is a promising probe of the early Universe. We study the effect of the gravitational wave background on the cosmic microwave background (CMB) and the large-scale structure (LSS) of the Universe. We find that the gravitational wave background can be detected by the CMB and the LSS if the mass of the black hole is larger than <math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"inline\"><mrow><mo>∼</mo><mn>10</mn><mtext> </mtext><mtext> </mtext><msub><mrow><mi>M</mi></mrow><mrow><mo stretchy=\"false\">⊙</mo></mrow></msub></mrow></math> and the black hole is located at a redshift of <math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"inline\"><mi>z</mi><mo>∼</mo><mn>10</mn></math>. The gravitational wave background can be detected by the CMB and the LSS if the black hole is located at a redshift of <math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"inline\"><mi>z</mi><mo>∼</mo><mn>10</mn></math> and the mass of the black hole is larger than <math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"inline\"><mo>∼</mo><mn>10</mn><mtext> </mtext><mtext> </mtext><msub><mi>M</mi><mo stretchy=\"false\">⊙</mo></msub></math>. The gravitational wave background can be detected by the CMB and the LSS if the black hole is located at a redshift of <math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"inline\"><mi>z</mi><mo>∼</mo><mn>10</mn></math> and the mass of the black hole is larger than <math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"inline\"><mo>∼</mo><mn>10</mn><mtext> </mtext><mtext> </mtext><msub><mi>M</mi><mo stretchy=\"false\n"
     ]
    }
   ],
   "source": [
    "# Input query - using the title of a paper in the dataset as an example\n",
    "text = \"The gravitational wave background from star-massive black hole fly-bys\"\n",
    "device = \"cpu\"\n",
    "# device = \"cuda:0\" \n",
    "inputs = tokenizer(text, return_tensors=\"pt\").to(device)\n",
    "\n",
    "# Generate output \n",
    "outputs = model.generate(**inputs, max_new_tokens=50)\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f177f40",
   "metadata": {},
   "source": [
    "# III. Finetuning The Model to the Dataset\n",
    "\n",
    "We first start with a Low-Rank Adaptation (LORA) config. This freezes the original model weights and adds on a small number of new parameters that will be fine-tuned. This makes fine-tuning significantly faster and more memory-efficient. It is pretty thoroughly proven in literature that fine-tuning the entire model is not necessary to achieve desired accuracy metrics. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d20948a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_config = LoraConfig(\n",
    "    r=8,\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"up_proj\", \"down_proj, gate_proj\"],\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5a5dea53",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 2549619/2549619 [17:14<00:00, 2463.57 examples/s]\n"
     ]
    }
   ],
   "source": [
    "data = ds.map(\n",
    "    lambda examples: tokenizer(examples[\"abstract\"]),\n",
    "    batched=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7a02cbf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['arxiv_id', 'title', 'authors', 'submission_date', 'comments', 'primary_subject', 'subjects', 'doi', 'abstract', 'file_path', 'input_ids', 'attention_mask']\n",
      "Stars on eccentric orbits around a massive black hole (MBH) emit bursts of gravitational waves (GWs) at periapse. Such events may be directly resolvable in the Galactic centre. However, if the star does not spiral in, the emitted GWs are not resolvable for extra-galactic MBHs, but constitute a source of background noise. We estimate the power spectrum of this extreme mass ratio burst background (EMBB) and compare it to the anticipated instrumental noise of the Laser Interferometer Space Antenna (LISA). To this end, we model the regions close to a MBH, accounting for mass-segregation, and for processes that limit the presence of stars close to the MBH, such as GW inspiral and hydrodynamical collisions between stars. We find that the EMBB is dominated by GW bursts from stellar mass black holes, and the magnitude of the noise spectrum (f S_GW)^{1/2} is at least a factor ~10 smaller than the instrumental noise. As an additional result of our analysis, we show that LISA is unlikely to detect relativistic bursts in the Galactic centre.\n"
     ]
    }
   ],
   "source": [
    "print(data[\"train\"].column_names)\n",
    "print(data[\"train\"][\"abstract\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "be4437f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def formatting_func(example):\n",
    "    \"\"\"\n",
    "    Format a mapped dataset example into a format we want to use for fine-tuning.\n",
    "    \"\"\"\n",
    "    text = f\"Title {example['title'][0]}\\nAbstract: {example['abstract'][0]}\\n\\n\"\n",
    "    return [text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "577d7081",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Benjamin\\.venv\\lm-311\\Lib\\site-packages\\bitsandbytes\\cextension.py:34: UserWarning: The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n",
      "  warn(\"The installed version of bitsandbytes was compiled without GPU support. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'NoneType' object has no attribute 'cadam32bit_grad_fp32'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Benjamin\\.venv\\lm-311\\Lib\\site-packages\\huggingface_hub\\file_download.py:942: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "'(ProtocolError('Connection aborted.', RemoteDisconnected('Remote end closed connection without response')), '(Request ID: 7570d678-d6d9-4038-bbd2-5346c1e7f68e)')' thrown while requesting HEAD https://huggingface.co/google/gemma-2b/resolve/main/tokenizer_config.json\n",
      "Retrying in 1s [Retry 1/5].\n",
      "c:\\Users\\Benjamin\\.venv\\lm-311\\Lib\\site-packages\\trl\\trainer\\sft_trainer.py:223: UserWarning: You didn't pass a `max_seq_length` argument to the SFTTrainer, this will default to 1024\n",
      "  warnings.warn(\n",
      "Map: 100%|██████████| 2549619/2549619 [00:54<00:00, 46985.31 examples/s] \n",
      "c:\\Users\\Benjamin\\.venv\\lm-311\\Lib\\site-packages\\trl\\trainer\\sft_trainer.py:290: UserWarning: You passed a tokenizer with `padding_side` not equal to `right` to the SFTTrainer. This might lead to some unexpected behaviour due to overflow issues when training a model in half-precision. You might consider adding `tokenizer.padding_side = 'right'` to your code.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Setup the training parameters, dataset, model, PEFT, etc.\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=data[\"train\"],\n",
    "    args=transformers.TrainingArguments(\n",
    "        per_device_train_batch_size=1,\n",
    "        gradient_accumulation_steps=4,\n",
    "        warmup_steps=2,\n",
    "        max_steps=100,\n",
    "        learning_rate=2e-4,\n",
    "        fp16=False,\n",
    "        # fp16=True,                    # Enable if running on CUDA GPUs\n",
    "        logging_steps=1,\n",
    "        output_dir=\"outputs\",\n",
    "        optim=\"paged_adamw_8bit\",\n",
    "    ),\n",
    "    peft_config=lora_config,\n",
    "    formatting_func=formatting_func,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6434d594",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "103e4f68",
   "metadata": {},
   "source": [
    "# IV. Testing the Fine-Tuned Model\n",
    "\n",
    "Now that we have fine-tuned the model, let's pass in the same input as before, and see if we get a more accurate output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40f517fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input query - using the title of a paper in the dataset as an example\n",
    "text = \"The gravitational wave background from star-massive black hole fly-bys\"\n",
    "device = \"cpu\"\n",
    "# device = \"cuda:0\" \n",
    "inputs = tokenizer(text, return_tensors=\"pt\").to(device)\n",
    "\n",
    "# Generate output \n",
    "outputs = model.generate(**inputs, max_new_tokens=50)\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9339108d",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lm-projects-env (3.11)",
   "language": "python",
   "name": "lm-311"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
